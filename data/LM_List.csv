Training Type,Notes, 200 - 300 M,500 - 600 M,700 - 800 M,1 - 2 B,2 - 3 B,3 - 4 B,6 - 7B,11 - 12 B,13 B,20 B,60 - 70 B,170 - 180 B
"Decoder Only, Pretrained",LM Pretrained Only,,,,EleutherAI/GPT-neo-1.3B,EleutherAI/GPT-neo-2.7B,,EleutherAI/GPT-J-6B,,,EleutherAI/GTP-NeoX-20B,,
,,,EleutherAI/pythia-410M,,EleutherAI/pythia-1b,EleutherAI/pythia-2.8b,,EleutherAI/pythia-6.9b,EleutherAI/pythia-12b,,,,
,,,EleutherAI/pythia-410M-deduped,,EleutherAI/pythia-1.4b,EleutherAI/pythia-2.8b-deduped,,EleutherAI/pythia-6.9b-deduped,EleutherAI/pythia-12b-deduped,,,,
,,,,,EleutherAI/pythia-1b-deduped,,,,,,,,
,,,,,EleutherAI/pythia-1.4b-deduped,,,,,,,,
"Decoder Only, Instruction Tuned",Cross lingual Instruction Tuned on xP3 (english -> 46 languages),,,,bigscience/bloomz-1b1,bigscience/bloomz-3b,,bigscience/bloomz-7b1,,,,,bigscience/bloomz
,,,,,bigscience/bloomz-1b7,,,,,,,,
,Crosslingual Instruction Tuned on xP3mt. 20 langs -> 46 langs,,,,,,,bigscience/bloomz-7b1-mt,,,,,bigscience/bloomz-mt
,Instruction tuned,,,,,,,togethercomputer/Pythia-Chat-Base-7B,,,togethercomputer/GPT-NeoXT-Chat-Base-20B,,
,,,,,,,,,,,,,
"Encoder-Decoder, Pretrained",Self-supervised denoising + supervised multitask training,t5-base,,t5-large,,t5-3B,,,t5-11B,,,,
,,google/t5-v1_1-base,,google/t5-v1_1-large,,google/t5-v1_1-xl,,,google/t5-v1_1-xxl,,,,
,Self-supervised denoising + supervised multitask training in 101 languages,google/mt5-base,,google/mt5-large,,google/mt5-xl,,,google/mt5-xxl,,,,
,,,,,,,,,,,google/ul2,,
"Encoder-Decoder, Instruction Tuned",instruction-tuned,google/flan-t5-base,,google/flan-t5-large,,google/flan-t5-xl,,,google/flan-t5-xxl,,,,
,,,,,,,,,,,google/flan-ul2,,
,,,,,,,,,bigscience/T0pp,,,,
,Cross lingual Instruction Tuned on xP3 (english -> 46 languages),,bigscience/mt0-base,,bigscience/mt0-large,,bigscience/mt0-xl,,,bigscience/mt0-xxl,,,
,Crosslingual Instruction Tuned on xP3mt. 20 langs -> 46 langs,,,,,,,,,bigscience/mt0-xxl-mt,,,